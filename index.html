<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="MotionPRO: Exploring the Role of Pressure in Human MoCap and Beyond. ">
  <!-- <meta name="keywords" content="SMPLX, Diffusion, Character Animation"> -->
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MotionPRO: Exploring the Role of Pressure in Human MoCap and Beyond</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- <h4 class="title conference-title is-4">NeurIPS 2023</h4> -->
            <h2 class="title is-2 publication-title">MotionPRO: Exploring the Role of Pressure in Human MoCap and Beyond</h2>
            <div class="is-size-5 publication-authors">
              
              <span class="author-block">
                <a href="https://www.wjrzm.com/">Shenghao Ren</a><sup>*1</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block">
                <a href="https://yeelou.github.io/">Yi Lu</a><sup>*1</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block">Jiayi Huang<sup>1</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block"><a href="zhaojy1022.com">Jiayi Zhao</a><sup>1</sup></span>
              <br />
              <span class="author-block">He Zhang<sup>3</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block"><a href="https://ytrock.com/">Tao Yu</a><sup>3</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block"><a href="https://shenqiu.njucite.cn/">Qiu Shen</a><sup>✉1,2</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block"><a href="https://cite.nju.edu.cn/People/Faculty/20190621/i5054.html">Xun Cao</a><sup>✉1,2</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
              <br />
             
              <!-- <span class="author-block"></span>
                <a>Anonymous Authors</a>
              </span> -->
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>School of Electronic Science and Engineering, Nanjing University, Nanjing, China</span> &nbsp;&nbsp;&nbsp;&nbsp;
              <span class="author-block"><sup>2</sup>Key Laboratory of Optoelectronic Devices and Systems with Extreme 
                <br />
                Performances of MOE, Nanjing University, Nanjing, China </span>
              <span class="author-block"><sup>3</sup>BNRist, Tsinghua University, Beijing, China</span> &nbsp;&nbsp;&nbsp;&nbsp;
            </div>
            <!-- <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>International Digital Economy Academy (IDEA),</span>
              <span class="author-block"><sup>2</sup>Shenzhen International Graduate School, Tsinghua University</span>
            </div> -->

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>*</sup>Equal Contribution &nbsp;&nbsp;</span>
              <span class="author-block"><sup>✉</sup>Corresponding Author</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">

                <!-- PDF Link. -->
                <span class="link-block">
                <a href="https://arxiv.org/abs/2504.05046" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>


                <!-- Video Link. -->
                <span class="link-block">
                <a href="https://youtu.be/UkUj3kiR5ss" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>

                <!-- Code Link.
                <span class="link-block">
                  <a href="https://github.com/Metaverse-AI-Lab-THU/MMVP-Dataset/tree/visualizing" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>MotionPRO-dataset</span>
                  </a>
                </span>


                <span class="link-block">
                  <a href="https://github.com/haolyuan/pressure_tookit" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>RGBD-fitting</span>
                  </a>
                </span> -->

                <span class="link-block">
                    <a href="https://github.com/wjrzm/MotionPRO" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <!-- https://docs.google.com/forms/d/e/1FAIpQLSf1uLLqXfnVXR2MoyRv5bF9D5gEMc8m8YASa38sTohzVcaVUg/viewform?usp=sf_link -->

                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://shenqiu.njucite.cn/download/" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fa fa-envelope"></i>
                    </span>
                    <span>Data Access</span>
                  </a>
                </span>


                <!-- Dataset Link. -->
                <!-- <span class="link-block">
                  <a href="https://drive.google.com" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
              </div> -->

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>




  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h3 class="title is-3 has-text-centered">Video</h3>

        <p align="middle">

          <video id="myVideo" src="./static/videos/v1.mp4" controls></video>

        </p>

      </div>
    </div>
  </section> -->

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h5 class="subtitle is-5">
            <b>MotionPRO</b> is a large-scale Human <b>Motion</b> capture dataset with <b>P</b>ressure, <b>R</b>GB  and <b>O</b>ptical sensors, which comprises 70 volunteers performing 400 types of motion, encompassing a total of 12.4M pose frames.
        </h5>
        <img src="./static/images/introduce.png" autoplay muted loop playsinline height="80%">
      </div>
    </div>
  </section>

  <p style="margin-bottom: -1.5cm;"></p>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            Existing human Motion Capture (MoCap) methods mostly focus on the visual similarity while neglecting the physical plausibility. As a result, downstream tasks such as driving virtual human in 3D scene or humanoid robots in real world suffer from issues such as timing drift and jitter, spatial problems like sliding and penetration, and poor global trajectory accuracy. In this paper, we revisit human MoCap from the perspective of interaction between human body and physical world by exploring the role of pressure. Firstly, we construct a large-scale Human <b>M</b>otion capture dataset with <b>P</b>ressure, <b>R</b>GB  and <b>O</b>ptical sensors (named <b>MotionPRO</b>), which comprises 70 volunteers performing 400 types of motion. Secondly, we examine both the necessity and effectiveness of the pressure signal through two challenging tasks: (1) pose and trajectory estimation based solely on pressure: We propose a network that incorporates a small kernel decoder and a long-short-term attention module, and proof that pressure could provide accurate global trajectory and plausible lower body pose.  (2) pose and trajectory estimation by fusing pressure and RGB: We impose constraints on orthographic similarity along the camera axis and whole-body contact along the vertical axis to enhance the cross-attention strategy to fuse pressure and RGB feature maps. Experiments demonstrate that fusing pressure with RGB features not only significantly improves performance in terms of objective metrics, but also plausibly drives virtual humans (SMPL) in 3D scene. Furthermore, we demonstrate that incorporating physical perception enables humanoid robots to perform more precise and stable actions, which is highly beneficial for the development of embodied artificial intelligence.
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
      

      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/UkUj3kiR5ss?si=aKcqkFT1v19NzPM1"
                    frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
      <!--/ Paper video. -->

    </div>
  </section>

  <!-- <head>
    <style>
        .video-container {
            display: flex;
            justify-content: center;
            align-items: center;
            height: 70vh; /* 70% of the viewport height */
        }
        .video-container iframe {
            width: 800px; /* 70% of the container's width */
            height: 450px; /* Maintain the aspect ratio */
        }
    </style>
</head>
<body>
    <div class="video-container">
        <h2 class="title is-3">Video</h2>
          <iframe src="https://www.youtube.com/embed/sksAVPmlDd8?si=GeTLhLQzzgthnOOr" frameborder="0" allowfullscreen></iframe>
    </div>
</body> -->




  <p style="margin-bottom: -0.5cm;"></p>

  <p style="margin-bottom: -0.5cm;"></p>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body has-text-centered">
        <h3 class="title is-3">Examples in MotionPRO</h3>
        <div class="videos-container">
          <div class="video-row">
            <video src="static/videos/video1_.mp4" autoplay muted loop></video>
            <video src="static/videos/video2_.mp4" autoplay muted loop></video>
          </div>
          <div class="video-row">
            <video src="static/videos/video3_.mp4" autoplay muted loop></video>
            <video src="static/videos/video4_.mp4" autoplay muted loop></video>
          </div>
        </div>
      </div>
    </div>
  </section>
  



  <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
    <source src="./static/videos/demo_video.gif" type="video/mp4">
  </video> -->



  <p style="margin-bottom: -1.5cm;"></p>
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content is-size-6">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{ren2025motionpro,
  title={MotionPRO: Exploring the Role of Pressure in Human MoCap and Beyond},
  author={Ren, Shenghao and Lu, Yi and Huang, Jiayi and Zhao, Jiayi and Zhang, He and Yu, Tao and Shen, Qiu and Cao, Xun},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={27760--27770},
  year={2025}
}</code></pre>

<!-- <h2 class="title">Contact Us</h2>
<style>
  ul {
    list-style-type: circle;
  }
</style>
<ul>
  <li>For detailed questions about this work, please contact Jing Lin (jinglin.stu@gmail.com).</li>
  <li>We are looking for talented, motivated, and creative research and engineering interns working on human-centric visual understanding and generation topics. If you are interested, please send your CV to Ailing Zeng (zengailing@idea.edu.cn).</li>
</ul> -->
    </div>
  </section>





  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-size-6">
          <div class="content">
            <p>
              This website is created with this <a href="https://motion-x-dataset.github.io/">template</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
